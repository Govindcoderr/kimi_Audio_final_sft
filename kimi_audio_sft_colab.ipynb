{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kimi-Audio Model Fine-Tuning with FSDP and LoRA\n",
    "## User-Friendly Version: Just provide your ZIP file path!\n",
    "\n",
    "This notebook handles:\n",
    "- Automatic ZIP extraction and dataset formatting\n",
    "- FSDP (Fully Sharded Data Parallel) training\n",
    "- LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
    "- Multi-GPU support\n",
    "- Automatic checkpoint saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h‚úÖ All dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "! pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "! pip install -q transformers datasets accelerate bitsandbytes peft\n",
    "! pip install -q wandb tensorboard soundfile librosa\n",
    "! pip install -q einops sentencepiece protobuf\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "Transformers version: 5.0.0\n",
      "CUDA available: True\n",
      "GPU count: 1\n",
      "  GPU 0: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp import ShardingStrategy, MixedPrecision\n",
    "from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "### üëâ MODIFY THESE PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "{\n",
      "  \"zip_file_path\": \"/content/drive/MyDrive/kimi_data/kimi_audio_dataset.zip\",\n",
      "  \"model_name\": \"moonshotai/Kimi-Audio-7B-Instruct\",\n",
      "  \"max_length\": 2048,\n",
      "  \"use_lora\": true,\n",
      "  \"lora_r\": 16,\n",
      "  \"lora_alpha\": 32,\n",
      "  \"lora_dropout\": 0.05,\n",
      "  \"lora_target_modules\": [\n",
      "    \"q_proj\",\n",
      "    \"k_proj\",\n",
      "    \"v_proj\",\n",
      "    \"o_proj\",\n",
      "    \"gate_proj\",\n",
      "    \"up_proj\",\n",
      "    \"down_proj\"\n",
      "  ],\n",
      "  \"use_fsdp\": true,\n",
      "  \"fsdp_sharding_strategy\": \"FULL_SHARD\",\n",
      "  \"fsdp_min_num_params\": 1000000.0,\n",
      "  \"output_dir\": \"./kimi_audio_finetuned\",\n",
      "  \"num_train_epochs\": 3,\n",
      "  \"per_device_train_batch_size\": 2,\n",
      "  \"per_device_eval_batch_size\": 2,\n",
      "  \"gradient_accumulation_steps\": 4,\n",
      "  \"learning_rate\": 0.0002,\n",
      "  \"warmup_steps\": 100,\n",
      "  \"logging_steps\": 10,\n",
      "  \"save_steps\": 500,\n",
      "  \"eval_steps\": 500,\n",
      "  \"save_total_limit\": 3,\n",
      "  \"fp16\": true,\n",
      "  \"bf16\": false,\n",
      "  \"train_split_ratio\": 0.9,\n",
      "  \"audio_sample_rate\": 16000,\n",
      "  \"extracted_dir\": \"./extracted_dataset\",\n",
      "  \"processed_dir\": \"./processed_dataset\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # ========== USER INPUT ==========\n",
    "    # Path to your ZIP file (local or Google Drive)\n",
    "    zip_file_path: str = \"/content/drive/MyDrive/kimi_data/kimi_audio_dataset.zip\"  # CHANGE THIS!\n",
    "    \n",
    "    # ========== MODEL SETTINGS ==========\n",
    "    model_name: str = \"moonshotai/Kimi-Audio-7B-Instruct\"  # or local path\n",
    "    max_length: int = 2048\n",
    "    \n",
    "    # ========== LoRA SETTINGS ==========\n",
    "    use_lora: bool = True\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: List[str] = field(default_factory=lambda: [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ])\n",
    "    \n",
    "    # ========== FSDP SETTINGS ==========\n",
    "    use_fsdp: bool = True\n",
    "    fsdp_sharding_strategy: str = \"FULL_SHARD\"  # or \"SHARD_GRAD_OP\", \"NO_SHARD\"\n",
    "    fsdp_min_num_params: int = 1e6  # Minimum parameters for wrapping\n",
    "    \n",
    "    # ========== TRAINING SETTINGS ==========\n",
    "    output_dir: str = \"./kimi_audio_finetuned\"\n",
    "    num_train_epochs: int = 3\n",
    "    per_device_train_batch_size: int = 2\n",
    "    per_device_eval_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    learning_rate: float = 2e-4\n",
    "    warmup_steps: int = 100\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 500\n",
    "    save_total_limit: int = 3\n",
    "    fp16: bool = True\n",
    "    bf16: bool = False  # Use bf16 if available (A100, H100)\n",
    "    \n",
    "    # ========== DATASET SETTINGS ==========\n",
    "    train_split_ratio: float = 0.9\n",
    "    audio_sample_rate: int = 16000\n",
    "    \n",
    "    # ========== PATHS (Auto-generated) ==========\n",
    "    extracted_dir: str = \"./extracted_dataset\"\n",
    "    processed_dir: str = \"./processed_dataset\"\n",
    "\n",
    "config = Config()\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(json.dumps(config.__dict__, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ZIP Extraction and Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Extracting ZIP file: /content/drive/MyDrive/kimi_data/kimi_audio_dataset.zip\n",
      "‚úÖ Extraction complete: extracted_dataset\n",
      "\n",
      "üìÅ Directory structure:\n",
      "extracted_dataset/\n",
      "  kimi_audio_dataset/\n",
      "    dataset.jsonl\n",
      "    Wav/\n",
      "      049.wav\n",
      "      050.wav\n",
      "      040.wav\n",
      "      007.wav\n",
      "      082.wav\n",
      "      ... and 72 more files\n",
      "\n",
      "üîç Detecting dataset format...\n",
      "‚úÖ Detected format: JSONL\n",
      "   Found 2 jsonl file(s)\n",
      "\n",
      "üìä Loaded 76 samples\n",
      "Sample data keys: ['audio', 'text']\n",
      "First sample: {\n",
      "  \"audio\": \"/content/Wav/035.wav\",\n",
      "  \"text\": \"will never mind the pies. As Mrs. Rachel says, pies they always were and pies they always will be, world without end amen.\"\n",
      "}...\n",
      "\n",
      "üîÑ Formatting data for training...\n",
      "‚úÖ Formatted 76 samples\n",
      "\n",
      "‚úÖ Dataset ready: 76 samples\n"
     ]
    }
   ],
   "source": [
    "class DatasetExtractor:\n",
    "    \"\"\"Handles ZIP extraction and dataset formatting\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.extracted_path = Path(config.extracted_dir)\n",
    "        self.processed_path = Path(config.processed_dir)\n",
    "        \n",
    "    def extract_zip(self):\n",
    "        \"\"\"Extract ZIP file to the extracted directory\"\"\"\n",
    "        print(f\"üì¶ Extracting ZIP file: {self.config.zip_file_path}\")\n",
    "        \n",
    "        if not os.path.exists(self.config.zip_file_path):\n",
    "            raise FileNotFoundError(f\"ZIP file not found: {self.config.zip_file_path}\")\n",
    "        \n",
    "        # Clean previous extraction\n",
    "        if self.extracted_path.exists():\n",
    "            shutil.rmtree(self.extracted_path)\n",
    "        self.extracted_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Extract\n",
    "        with zipfile.ZipFile(self.config.zip_file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.extracted_path)\n",
    "        \n",
    "        print(f\"‚úÖ Extraction complete: {self.extracted_path}\")\n",
    "        self._print_directory_structure()\n",
    "        \n",
    "    def _print_directory_structure(self):\n",
    "        \"\"\"Print the extracted directory structure\"\"\"\n",
    "        print(\"\\nüìÅ Directory structure:\")\n",
    "        for root, dirs, files in os.walk(self.extracted_path):\n",
    "            level = root.replace(str(self.extracted_path), '').count(os.sep)\n",
    "            indent = ' ' * 2 * level\n",
    "            print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            sub_indent = ' ' * 2 * (level + 1)\n",
    "            for file in files[:5]:  # Show first 5 files\n",
    "                print(f\"{sub_indent}{file}\")\n",
    "            if len(files) > 5:\n",
    "                print(f\"{sub_indent}... and {len(files) - 5} more files\")\n",
    "    \n",
    "    def detect_format(self) -> str:\n",
    "        \"\"\"Auto-detect dataset format\"\"\"\n",
    "        print(\"\\nüîç Detecting dataset format...\")\n",
    "        \n",
    "        # Check for common formats\n",
    "        formats = {\n",
    "            'jsonl': list(self.extracted_path.rglob('*.jsonl')),\n",
    "            'json': list(self.extracted_path.rglob('*.json')),\n",
    "            'csv': list(self.extracted_path.rglob('*.csv')),\n",
    "            'txt': list(self.extracted_path.rglob('*.txt')),\n",
    "        }\n",
    "        \n",
    "        for fmt, files in formats.items():\n",
    "            if files:\n",
    "                print(f\"‚úÖ Detected format: {fmt.upper()}\")\n",
    "                print(f\"   Found {len(files)} {fmt} file(s)\")\n",
    "                return fmt\n",
    "        \n",
    "        # Check for audio files\n",
    "        audio_files = list(self.extracted_path.rglob('*.wav')) + \\\n",
    "                     list(self.extracted_path.rglob('*.mp3')) + \\\n",
    "                     list(self.extracted_path.rglob('*.flac'))\n",
    "        \n",
    "        if audio_files:\n",
    "            print(f\"‚úÖ Detected audio dataset with {len(audio_files)} files\")\n",
    "            return 'audio'\n",
    "        \n",
    "        raise ValueError(\"Could not detect dataset format. Please check your ZIP structure.\")\n",
    "    \n",
    "    def load_dataset(self) -> List[Dict]:\n",
    "        \"\"\"Load dataset based on detected format\"\"\"\n",
    "        fmt = self.detect_format()\n",
    "        data = []\n",
    "        \n",
    "        if fmt == 'jsonl':\n",
    "            for file in self.extracted_path.rglob('*.jsonl'):\n",
    "                with open(file, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        data.append(json.loads(line.strip()))\n",
    "        \n",
    "        elif fmt == 'json':\n",
    "            for file in self.extracted_path.rglob('*.json'):\n",
    "                with open(file, 'r', encoding='utf-8') as f:\n",
    "                    content = json.load(f)\n",
    "                    if isinstance(content, list):\n",
    "                        data.extend(content)\n",
    "                    else:\n",
    "                        data.append(content)\n",
    "        \n",
    "        elif fmt == 'audio':\n",
    "            # For audio datasets, create metadata from audio files\n",
    "            audio_files = list(self.extracted_path.rglob('*.wav')) + \\\n",
    "                         list(self.extracted_path.rglob('*.mp3')) + \\\n",
    "                         list(self.extracted_path.rglob('*.flac'))\n",
    "            \n",
    "            for audio_file in audio_files:\n",
    "                # Try to find corresponding text/transcript\n",
    "                txt_file = audio_file.with_suffix('.txt')\n",
    "                transcript = \"\"\n",
    "                if txt_file.exists():\n",
    "                    with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "                        transcript = f.read().strip()\n",
    "                \n",
    "                data.append({\n",
    "                    'audio_path': str(audio_file),\n",
    "                    'text': transcript,\n",
    "                    'filename': audio_file.name\n",
    "                })\n",
    "        \n",
    "        print(f\"\\nüìä Loaded {len(data)} samples\")\n",
    "        if data:\n",
    "            print(f\"Sample data keys: {list(data[0].keys())}\")\n",
    "            print(f\"First sample: {json.dumps(data[0], indent=2, default=str)[:500]}...\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def format_for_training(self, data: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Format data for Kimi-Audio training\"\"\"\n",
    "        print(\"\\nüîÑ Formatting data for training...\")\n",
    "        formatted_data = []\n",
    "        \n",
    "        for item in data:\n",
    "            # Flexible key mapping\n",
    "            text_keys = ['text', 'transcript', 'transcription', 'label', 'target']\n",
    "            audio_keys = ['audio', 'audio_path', 'file', 'path']\n",
    "            \n",
    "            text = None\n",
    "            for key in text_keys:\n",
    "                if key in item:\n",
    "                    text = item[key]\n",
    "                    break\n",
    "            \n",
    "            audio_path = None\n",
    "            for key in audio_keys:\n",
    "                if key in item:\n",
    "                    audio_path = item[key]\n",
    "                    break\n",
    "            \n",
    "            if text:  # At minimum we need text\n",
    "                formatted_item = {\n",
    "                    'text': str(text),\n",
    "                }\n",
    "                if audio_path:\n",
    "                    formatted_item['audio_path'] = str(audio_path)\n",
    "                \n",
    "                formatted_data.append(formatted_item)\n",
    "        \n",
    "        print(f\"‚úÖ Formatted {len(formatted_data)} samples\")\n",
    "        return formatted_data\n",
    "\n",
    "# Execute extraction\n",
    "extractor = DatasetExtractor(config)\n",
    "extractor.extract_zip()\n",
    "raw_data = extractor.load_dataset()\n",
    "formatted_data = extractor.format_for_training(raw_data)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset ready: {len(formatted_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "class KimiAudioDataset(Dataset):\n",
    "    \"\"\"Custom dataset for Kimi-Audio fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict], tokenizer, max_length: int = 2048):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['text']\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': input_ids.clone()  # For causal LM\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace with your actual token from https://huggingface.co/settings/tokens\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m130.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.9.0+cu128)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl size=253780426 sha256=4e2f9e39313266b1544b68138b15b91ee6221eccf14f7902b7c6620351340810\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/59/46/f282c12c73dd4bb3c2e3fe199f1a0d0f8cec06df0cccfeee27\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: flash-attn\n",
      "Successfully installed flash-attn-2.8.3\n",
      "‚úÖ FlashAttention installed successfully!\n"
     ]
    }
   ],
   "source": [
    "! pip install flash-attn --no-build-isolation\n",
    "print(\"‚úÖ FlashAttention installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831cf8e2d7b14da1acbda0654cbea723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0130b88eef64c658dbe8cfe0415e806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_moonshot_kimia.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/moonshotai/Kimi-Audio-7B-Instruct:\n",
      "- configuration_moonshot_kimia.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf4d870f416480e8af303af1480f377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc01a32f405d45eb8c5a66c0c9d1d157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_kimia.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/moonshotai/Kimi-Audio-7B-Instruct:\n",
      "- tokenization_kimia.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d971f36e46a4072a95b822576834261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tiktoken.model:   0%|          | 0.00/2.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacf9bf9a4c14283ac754fb0b01e8bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer loaded\n",
      "Vocab size: 152064\n",
      "\n",
      "üîÑ Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd5f801785647b9bf11b7a6a244b4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_moonshot_kimia.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/moonshotai/Kimi-Audio-7B-Instruct:\n",
      "- modeling_moonshot_kimia.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "flash attention must be installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3566827934.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüîÑ Loading base model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_remote_code\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             model_class = get_class_from_dynamic_module(\n\u001b[0m\u001b[1;32m    355\u001b[0m                 \u001b[0mclass_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_revision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcode_revision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/dynamic_module_utils.py\u001b[0m in \u001b[0;36mget_class_from_dynamic_module\u001b[0;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     )\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_class_in_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/dynamic_module_utils.py\u001b[0m in \u001b[0;36mget_class_in_module\u001b[0;34m(class_name, module_path, force_reload)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# reload in both cases, unless the module is already imported and the hash hits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__transformers_module_hash__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmodule_hash\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mmodule_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__transformers_module_hash__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/moonshotai/Kimi_hyphen_Audio_hyphen_7B_hyphen_Instruct/9a82a84c37ad9eb1307fb6ed8d7b397862ef9e6b/modeling_moonshot_kimia.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mflash_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_padding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mindex_first_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpad_input\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"flash attention must be installed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: flash attention must be installed"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "# Set padding token if not exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"‚úÖ Tokenizer loaded\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "# print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "\n",
    "print(\"\\nüîÑ Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if config.fp16 else torch.float32,\n",
    "    device_map='auto',\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     config.model_name,\n",
    "#     trust_remote_code=True,\n",
    "#     dtype=torch.float16 if config.fp16 else torch.float32,\n",
    "#     device_map=\"auto\",\n",
    "#     # low_cpu_mem_usage=True,\n",
    "#     attn_implementation=\"eager\"   # üëà disables flash attention\n",
    "# )\n",
    "\n",
    "\n",
    "print(\"‚úÖ Model loaded\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 11 11:53:49 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   44C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Applying LoRA configuration...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4163656083.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_trainable_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "if config.use_lora:\n",
    "    print(\"\\nüîÑ Applying LoRA configuration...\")\n",
    "    \n",
    "    # Prepare model for k-bit training (if using quantization)\n",
    "    # model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        target_modules=config.lora_target_modules,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    print(\"‚úÖ LoRA applied successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è LoRA is disabled - training full model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "split_idx = int(len(formatted_data) * config.train_split_ratio)\n",
    "train_data = formatted_data[:split_idx]\n",
    "eval_data = formatted_data[split_idx:]\n",
    "\n",
    "print(f\"üìä Dataset split:\")\n",
    "print(f\"  Training samples: {len(train_data)}\")\n",
    "print(f\"  Evaluation samples: {len(eval_data)}\")\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = KimiAudioDataset(train_data, tokenizer, config.max_length)\n",
    "eval_dataset = KimiAudioDataset(eval_data, tokenizer, config.max_length) if eval_data else None\n",
    "\n",
    "print(\"‚úÖ Datasets created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configure FSDP Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSDP configuration\n",
    "fsdp_config = None\n",
    "if config.use_fsdp and torch.cuda.device_count() > 1:\n",
    "    fsdp_config = {\n",
    "        \"fsdp_transformer_layer_cls_to_wrap\": [\"LlamaDecoderLayer\"],  # Adjust based on model\n",
    "        \"fsdp_sharding_strategy\": config.fsdp_sharding_strategy,\n",
    "        \"fsdp_auto_wrap_policy\": \"TRANSFORMER_BASED_WRAP\",\n",
    "        \"fsdp_backward_prefetch\": \"BACKWARD_PRE\",\n",
    "        \"fsdp_cpu_ram_efficient_loading\": True,\n",
    "        \"fsdp_state_dict_type\": \"FULL_STATE_DICT\",\n",
    "    }\n",
    "    print(\"‚úÖ FSDP configuration prepared\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è FSDP disabled (single GPU or disabled in config)\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    num_train_epochs=config.num_train_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    logging_dir=f\"{config.output_dir}/logs\",\n",
    "    logging_steps=config.logging_steps,\n",
    "    save_steps=config.save_steps,\n",
    "    eval_steps=config.eval_steps,\n",
    "    save_total_limit=config.save_total_limit,\n",
    "    fp16=config.fp16,\n",
    "    bf16=config.bf16,\n",
    "    evaluation_strategy=\"steps\" if eval_dataset else \"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True if eval_dataset else False,\n",
    "    metric_for_best_model=\"loss\" if eval_dataset else None,\n",
    "    greater_is_better=False,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    remove_unused_columns=False,\n",
    "    fsdp=fsdp_config if fsdp_config else None,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We're doing causal LM, not masked LM\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized and ready\")\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  LoRA: {config.use_lora}\")\n",
    "print(f\"  FSDP: {config.use_fsdp}\")\n",
    "print(f\"  Epochs: {config.num_train_epochs}\")\n",
    "print(f\"  Batch size: {config.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {config.per_device_train_batch_size * config.gradient_accumulation_steps * torch.cuda.device_count()}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Output directory: {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Start Training üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ TRAINING COMPLETED\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Save metrics\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "print(f\"Training metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüíæ Saving final model...\")\n",
    "\n",
    "# Save model\n",
    "final_model_path = os.path.join(config.output_dir, \"final_model\")\n",
    "trainer.save_model(final_model_path)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {final_model_path}\")\n",
    "\n",
    "# If using LoRA, save adapter separately\n",
    "if config.use_lora:\n",
    "    lora_path = os.path.join(config.output_dir, \"lora_adapter\")\n",
    "    model.save_pretrained(lora_path)\n",
    "    print(f\"‚úÖ LoRA adapter saved to: {lora_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Run Evaluation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_dataset:\n",
    "    print(\"\\nüìä Running final evaluation...\")\n",
    "    \n",
    "    eval_metrics = trainer.evaluate()\n",
    "    \n",
    "    print(f\"\\nEvaluation metrics:\")\n",
    "    for key, value in eval_metrics.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    trainer.log_metrics(\"eval\", eval_metrics)\n",
    "    trainer.save_metrics(\"eval\", eval_metrics)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No evaluation dataset available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüß™ Testing inference with fine-tuned model...\")\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"Hello, how are you?\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# Decode\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\nPrompt: {test_prompt}\")\n",
    "print(f\"Generated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up temporary files\n",
    "# print(\"\\nüßπ Cleaning up temporary files...\")\n",
    "# shutil.rmtree(config.extracted_dir, ignore_errors=True)\n",
    "# print(\"‚úÖ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã TRAINING SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n‚úÖ Fine-tuning completed successfully!\\n\")\n",
    "\n",
    "print(f\"Dataset Information:\")\n",
    "print(f\"  ZIP file: {config.zip_file_path}\")\n",
    "print(f\"  Training samples: {len(train_data)}\")\n",
    "print(f\"  Evaluation samples: {len(eval_data)}\")\n",
    "\n",
    "print(f\"\\nModel Information:\")\n",
    "print(f\"  Base model: {config.model_name}\")\n",
    "print(f\"  LoRA enabled: {config.use_lora}\")\n",
    "if config.use_lora:\n",
    "    print(f\"  LoRA rank: {config.lora_r}\")\n",
    "    print(f\"  LoRA alpha: {config.lora_alpha}\")\n",
    "print(f\"  FSDP enabled: {config.use_fsdp}\")\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"  Epochs: {config.num_train_epochs}\")\n",
    "print(f\"  Batch size: {config.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Precision: {'FP16' if config.fp16 else 'BF16' if config.bf16 else 'FP32'}\")\n",
    "\n",
    "print(f\"\\nOutput Locations:\")\n",
    "print(f\"  Model directory: {final_model_path}\")\n",
    "if config.use_lora:\n",
    "    print(f\"  LoRA adapter: {lora_path}\")\n",
    "print(f\"  Checkpoints: {config.output_dir}\")\n",
    "print(f\"  Logs: {config.output_dir}/logs\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ You can now use your fine-tuned model!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTo load the model later:\")\n",
    "print(f\"```python\")\n",
    "print(f\"from transformers import AutoModelForCausalLM, AutoTokenizer\")\n",
    "print(f\"model = AutoModelForCausalLM.from_pretrained('{final_model_path}')\")\n",
    "print(f\"tokenizer = AutoTokenizer.from_pretrained('{final_model_path}')\")\n",
    "print(f\"```\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
